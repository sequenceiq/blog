<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hadoop | SequenceIQ Blog]]></title>
  <link href="http://blog.sequenceiq.com/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://blog.sequenceiq.com/"/>
  <updated>2014-05-26T15:52:58+00:00</updated>
  <id>http://blog.sequenceiq.com/</id>
  <author>
    <name><![CDATA[SequenceIQ]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Apache Ambari + Spring Shell = Ambari Shell]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/05/26/ambari-shell/"/>
    <updated>2014-05-26T13:42:11+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/05/26/ambari-shell</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p><a href="http://ambari.apache.org/">Apache Ambari&rsquo;s</a> goal is to make Hadoop cluster management as simple as possible. It provides an intuitive easy-to-use
web UI backed by its RESTful API. With only a few clicks you&rsquo;re able to install Hadoop services across any number of hosts and Ambari will take
care of the configurations as well. After the installation is complete you can monitor your cluster taking leverage of
<a href="http://ganglia.sourceforge.net/">Ganglia</a> and <a href="http://www.nagios.org/">Nagios</a>. At SequenceIQ we use command line tools whenever it&rsquo;s possible,
because it&rsquo;s much faster than interacting with a web UI and it&rsquo;s a better candidate for automation. Here comes
<a href="https://github.com/spring-projects/spring-shell#readme">Spring Shell</a> to our rescue. An interactive shell that can be easily extended
using a Spring based programming model and battle tested in various projects like <a href="http://projects.spring.io/spring-roo/">Spring Roo</a> and
<a href="http://docs.spring.io/spring-xd/docs/1.0.0.BUILD-SNAPSHOT/reference/html/">Spring XD</a>. Combine these two projects and a really powerful tool
will come to light.</p>

<h2>Ambari Shell</h2>

<p>The goal is to provide an interactive command line tool which supports:</p>

<ul>
<li>all functionality available through the Ambari web UI</li>
<li>context aware command availability</li>
<li>tab completion</li>
<li>required/optional parameter support</li>
</ul>


<p>Since we&rsquo;re open sourcing the project, it should be available and part of the official Ambari repository <a href="https://issues.apache.org/jira/browse/AMBARI-5482">soon</a>,
but if you&rsquo;re eager to try it you can build your own from our <a href="https://github.com/sequenceiq/ambari-shell">repository</a> (mvn clean install).
The shell is distributed as a single executable jar with the help of another project called <a href="http://projects.spring.io/spring-boot/">Spring Boot</a>.
Let&rsquo;s see how it works in real life.</p>

<!-- more -->


<p>As usual we&rsquo;ve crated a <a href="https://github.com/sequenceiq/ambari-docker">Docker</a> image so you can start experimenting with the shell and it&rsquo;s
available at the Docker repository, which means you only need to run the following to get a running Ambari server:
<code>
docker run -d -P -h server.ambari.com --name ambari-singlenode sequenceiq/ambari
</code>
and you can connect to it with the shell:
```
Usage:
  java -jar ambari-shell.jar                  : Starts Ambari Shell in interactive mode.
  java -jar ambari-shell.jar &mdash;cmdfile=<FILE> : Ambari Shell executes commands read from the file.</p>

<p>Options:
  &mdash;ambari.host=<HOSTNAME>       Hostname of the Ambari Server [default: localhost].
  &mdash;ambari.port=<PORT>           Port of the Ambari Server [default: 8080].
  &mdash;ambari.user=<USER>           Username of the Ambari admin [default: admin].
  &mdash;ambari.password=<PASSWORD>   Password of the Ambari admin [default: admin].</p>

<p>Note:
  At least one option is mandatory.
<code>
The `--ambari` options can be omitted if the values are the defaults otherwise you only need to specify the difference,
e.g just the port is different: `--ambari.port=49178`.
</code></p>

<pre><code>    _                _                   _  ____   _            _  _
</code></pre>

<p>   / \    _ <strong> </strong><em>  | |<strong>    </strong> _  _ __ (</em>)/ <em><strong>| | |</strong>    </em><strong> | || |
  / _ \  | &lsquo;_ <code>_ \ | '_ \  / _</code> || &rsquo;</strong>|| |_<strong> \ | &lsquo;<em> \  / _ \| || |
 / </em></strong> \ | | | | | || |<em>) || (</em>| || |   | | <em><strong>) || | | ||  </strong>/| || |
/</em>/   _\|<em>| |</em>| |<em>||</em>.<strong>/  _<em>,</em>||<em>|   |</em>||</strong><strong>/ |<em>| |</em>| _</strong>||<em>||</em>|</p>

<p>Welcome to Ambari Shell. For assistance press tab or use the <code>hint</code> command.
```
The currently supported commands are:</p>

<ul>
<li><code>blueprint add</code> &ndash; Add a new blueprint with either &mdash;url or &mdash;file</li>
<li><code>blueprint defaults</code> &ndash; Adds the default blueprints to Ambari</li>
<li><code>blueprint list</code> &ndash; Lists all known blueprints</li>
<li><code>blueprint show</code> &ndash; Shows the blueprint by its id</li>
<li><code>cluster assign</code> &ndash; Assign host to host group</li>
<li><code>cluster build</code> &ndash; Starts to build a cluster</li>
<li><code>cluster create</code> &ndash; Create a cluster based on current blueprint and assigned hosts</li>
<li><code>cluster delete</code> &ndash; Delete the cluster</li>
<li><code>cluster preview</code> &ndash; Shows the currently assigned hosts</li>
<li><code>cluster reset</code> &ndash; Clears the host &ndash; host group assignments</li>
<li><code>debug off</code> &ndash; Stops showing the URL of the API calls</li>
<li><code>debug on</code> &ndash; Shows the URL of the API calls</li>
<li><code>exit</code> &ndash; Exits the shell</li>
<li><code>hello</code> &ndash; Prints a simple elephant to the console</li>
<li><code>help</code> &ndash; List all commands usage</li>
<li><code>hint</code> &ndash; Shows some hints</li>
<li><code>host components</code> &ndash; Lists the components assigned to the selected host</li>
<li><code>host focus</code> &ndash; Sets the useHost to the specified host</li>
<li><code>host list</code> &ndash; Lists the available hosts</li>
<li><code>quit</code> &ndash; Exits the shell</li>
<li><code>script</code> &ndash; Parses the specified resource file and executes its commands</li>
<li><code>service components</code> &ndash; Lists all services with their components</li>
<li><code>service list</code> &ndash; Lists the available services</li>
<li><code>tasks</code> &ndash; Lists the Ambari tasks</li>
<li><code>version</code> &ndash; Displays shell version</li>
</ul>


<p>All commands are context aware and are available only when it makes sense. For example the <code>cluster create</code> command is not available
until a blueprint hasn&rsquo;t been added or selected. A good approach is to use the <code>hint</code> command &ndash; as the Ambari UI, this will give
you hints about the available commands and the flow of creating or configuring a cluster. You can always use TAB for completion
or available parameters. Be nice and say <code>hello</code>:
```</p>

<pre><code>            .-.._
      __  /`     '.
   .-'  `/   (   a \
  /      (    \,_   \
 /|       '---` |\ =|
` \    /__.-/  /  | |
   |  / / \ \  \   \_\
   |__|_|  |_|__\
</code></pre>

<p><code>
Initially there are no blueprints available - you can add blueprints from file or URL. For your convenience we've added two
blueprints as defaults. You can get these blueprints by using the `blueprint defaults` command. The result is the following:
</code>
  BLUEPRINT              STACK</p>

<hr />

<p>  multi-node-hdfs-yarn   HDP:2.0
  single-node-hdfs-yarn  HDP:2.0
<code>
Once the blueprints are added you can use them to create a cluster by typing `cluster build --blueprint single-node-hdfs-yarn`.
Now that the blueprint is selected you have to assign the hosts to the available host groups. Use
`cluster assign --hostGroup host_group_1 --host server.ambari.com`.
</code>
  HOSTGROUP     HOST</p>

<hr />

<p>  host_group_1  server.ambari.com
<code>
Once you are happy with the host - host group associations you can choose `cluster create` to start building the cluster.
Progress can be checked either at the Amabri UI or using the `tasks` command.
</code>
  TASK                        STATUS</p>

<hr />

<p>  HISTORYSERVER INSTALL       QUEUED
  ZOOKEEPER_SERVER START      PENDING
  ZOOKEEPER_CLIENT INSTALL    PENDING
  HDFS_CLIENT INSTALL         PENDING
  HISTORYSERVER START         PENDING
  NODEMANAGER INSTALL         QUEUED
  NODEMANAGER START           PENDING
  ZOOKEEPER_SERVER INSTALL    QUEUED
  YARN_CLIENT INSTALL         PENDING
  NAMENODE INSTALL            QUEUED
  RESOURCEMANAGER INSTALL     QUEUED
  NAMENODE START              PENDING
  RESOURCEMANAGER START       PENDING
  DATANODE START              PENDING
  SECONDARY_NAMENODE START    PENDING
  DATANODE INSTALL            QUEUED
  MAPREDUCE2_CLIENT INSTALL   PENDING
  SECONDARY_NAMENODE INSTALL  QUEUED
```</p>

<p>Each time you start the shell the executed commands are logged in a file line by line and later either with the <code>script</code> command
or specifying an <code>--cmdfile</code> option the same commands can be executed.</p>

<h2>Summary</h2>

<p>To sum it up in less than two minutes watch this video:</p>

<script type="text/javascript" src="https://asciinema.org/a/9783.js" id="asciicast-9783" async></script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Job profiling with R]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/05/01/mapreduce-job-profiling-with-R/"/>
    <updated>2014-05-01T21:08:04+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/05/01/mapreduce-job-profiling-with-R</id>
    <content type="html"><![CDATA[<p>Management of a large Hadoop cluster is not an easy task &ndash; however thanks to projects like <a href="http://ambari.apache.org/">Apache Ambari</a> these tasks are getting easier. Ambari provides an intuitive, easy-to-use Hadoop management web UI backed by its REST API to provision, manage and monitor a Hadoop cluster. While Ambari helps us a lot to monitor a cluster (leverages <a href="http://ganglia.sourceforge.net/">Ganglia</a> and <a href="http://www.nagios.org/">Nagios</a>), many times we have to profile our MapReduce jobs as well.</p>

<p>At SequenceIQ in order to profile MapReduce jobs, understand (job)internal statistics and create usefull graphs many times we rely on <a href="http://www.r-project.org/">R</a>. The metrics are collected from Ambari and the <a href="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/HistoryServerRest.html">YARN History Server</a>.</p>

<p>In this blog post we would like to explain and guide you through a simple process of collecting MapReduce job metrics, calculate different statistics and generate easy to understand charts.</p>

<p>The MapReduce application is the following:</p>

<ul>
<li>The input set of data is 12 pieces of 1 GB size files. Each file containes the same line of 16 bytes (012345678998765 plus the new line character)</li>
<li>The number of mappers running is 48, because the block size on HDFS is 256 MB and there are 12 files.</li>
<li>We use TextInputFormat (line num, line content) pairs. The output of the mapper function is the same as the input <code>IdentityMapper</code></li>
<li>The number of reducers is 20.</li>
<li>For simplicity we use <code>IdentityReducer</code> as the reducer function.</li>
<li>We use a special partitioner called <code>LinePartitoner</code>. The partitioning is based on line numbers (the key) and it makes sure that each reducer gets the same amount of data (line number <em>modulo</em> reducer number).</li>
</ul>


<h2>How to get the job results with R</h2>

<p>The job id that we are analysing with R is job_1395530889914_0005 (<em>replace this with your job is</em>)</p>

<p>First we load the R functions:</p>

<p><code>source("JobHistory.r")</code></p>

<p>Then we extract/read the job from the HistoryServer. It is actually using the Rest API of HistoryServer, parsing the JSON output.</p>

<p><code>job&lt;-getJob("job_1395530889914_0005","node02:19888")</code></p>

<p>The structure of the job follows the structure that is returned from the HistoryServer except that for example the parameters of all the tasks are converted into vectors so that can be easily handled in R.</p>

<!-- more -->


<p>A job is a list of <code>things</code>:</p>

<p><code>&gt; names(job)</code></p>

<p><code>[1] "job"      "counters" "tasks"    "attempts"</code></p>

<p>The job$job contains some basic data</p>

<p><code>&gt; names(job$job)</code></p>

<p><code>[1] "startTime"                "finishTime"               "id"                       "name"                     "queue"</code></p>

<p><code>[6] "user"                     "state"                    "mapsTotal"                "mapsCompleted"            "reducesTotal"</code></p>

<p><code>[11] "reducesCompleted"         "uberized"                 "diagnostics"              "avgMapTime"               "avgReduceTime"</code></p>

<p><code>[16] "avgShuffleTime"           "avgMergeTime"             "failedReduceAttempts"     "killedReduceAttempts"     "successfulReduceAttempts"</code></p>

<p><code>[21] "failedMapAttempts"        "killedMapAttempts"        "successfulMapAttempts"</code></p>

<p>The items below job$tasks are all vectors (if there are numeric) or non-named lists:</p>

<p><code>&gt; names(job$tasks)</code></p>

<p><code>[1] "startTime"         "finishTime"        "elapsedTime"       "progress"          "id"          "state"             "type"</code></p>

<p><code>[8] "successfulAttempt"</code></p>

<p>This way we can easily calculate the mean of the <code>running</code> times of all the tasks like this:</p>

<p><code>mean(job$tasks$finishTime-job$tasks$startTime)</code></p>

<p><code>[1] 147307</code></p>

<p>The <code>attempts</code> list also contains vectors or lists of parameters. Only the successful attempts are in the attempt list.</p>

<p><code>&gt; names(job$attempts)</code></p>

<p><code>[1] "startTime"           "finishTime"          "elapsedTime"         "progress"            "id"                  "rack"</code></p>

<p><code>[7] "state"               "nodeHttpAddress"     "diagnostics"         "type"                "assignedContainerId" "shuffleFinishTime"</code></p>

<p><code>[13] "mergeFinishTime"     "elapsedShuffleTime"  "elapsedMergeTime"    "elapsedReduceTime"</code></p>

<p>This way we can easily calculate the average <code>merge</code> times:</p>

<p><code>&gt; mean(job$attempts$mergeFinishTime-job$attempts$shuffleFinishTime)</code></p>

<p><code>[1] 4875.15</code></p>

<p>Which is the same as:</p>

<p><code>&gt; mean(job$attempts$elapsedMergeTime)</code></p>

<p><code>[1] 4875.15</code></p>

<h2>The R generated graphs</h2>

<p>The are two types of graphs for the beginning</p>

<p><code>plotTasksTimes(job)</code></p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/yarn-monitoring-R/images/48_mappers_20_reducers_mr_task_times.png" alt="" /></p>

<p>This graph shows start and finish times for each tasks (mappers and reducers as well). The tasks are sorted by their start times, so the reducers are on the top. There are 48 mappers and 20 reducers. The times are relative to the startTime of the first mapper in milliseconds(could show absolute values as well).</p>

<p><code>plotActiveMRTasksNum(job)</code></p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/yarn-monitoring-R/images/48_mappers_20_reducers_mr.png" alt="" /></p>

<p>The graph above contains the number of active tasks at each time. It shows the mappers with green and also show the reduce phases as well. The shuffle part is orange, the merge part is magenta and the reduce part (reducer function is running) is blue. The times are relative to the startTime of the first mapper in milliseconds (could show absolute values as well).</p>

<p><code>plotActiveReduceTasksNumDetailed(job, FALSE)</code></p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/yarn-monitoring-R/images/48_mappers_20_reducers_reduce_phases.png" alt="" /></p>

<p>This graph shows only the reduce part with the three phases: shuffle, merge, reduce. The times are absolute times (could show absolute values as well).</p>

<p><code>plotTimeBoxes&lt;-function(data, nodeNum=21, slotsPerNode=4)</code></p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/yarn-monitoring-R/images/48_mappers_20_reducers_mr_by_nodes.png" alt="" /></p>

<p>As you can see monitoring a MapReduce job through the HistoryServer it is extremely easy, and R is very usefull to apply different statistics and plot graphs. Also as you start playing with different setups the results can quickly be retrived, the graphs regenerated to analyze how different configuratins are affecting the execution time/behaviour of the jobs.</p>

<p><img src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/yarn-monitoring-R/images/96_mappers_20_reducers_mr_by_nodes.png" alt="" /></p>

<p>As always, the example project is available at our <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/yarn-monitoring-R">GitHub</a> page. We are working on a <code>heuristic</code> queue scheduler for a better utilization of our cluster, and also to provide QoS on Hadoop &ndash; profiling and understanding the running MapReduce jobs and the job queues are essential for that. Also based on the charts broken down by nodes we can quickly identify servers with potential issues (slow I/O, memory, etc).</p>

<p>Follow us on <a href="https://www.linkedin.com/company/sequenceiq/">LinkedIn</a> to read about how we progress with the sceduler and get early access, or feel free to contribute to our YARN monitoring project.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Writing MapReduce jobs in Scala]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/04/14/mapreduce-with-scalding/"/>
    <updated>2014-04-14T11:55:38+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/04/14/mapreduce-with-scalding</id>
    <content type="html"><![CDATA[<p>At SequenceIQ we have many pre-built and configurable MapReduce jobs (complex math algorithms, filtering, sorting and correlation patterns, samplings, top-n, joins, partitioning, etc) &ndash; as building blocks of our job worklow. We needed to find a quick way to build and test these jobs during developement on &lsquo;local&rsquo; mode and be able to push the same jobs to a large test cluster without any modifications.
Though in general we use Java, we always strive for efficiency when we need to solve a problem and we use different  languages (not just JVM based) in our stack (e.g. Groovy, Go and R) &ndash; to write MapReduce jobs we have choosen Scala and the Scalding library. Scalding is a Scala library developed by Twitter that abstracts and makes easy to write Hadoop MapReduce jobs. In many ways Scalding is similar to Pig, but it was writen in Scala, bringing the advantages of Scala to your MapReduce jobs (e.g. type safety &ndash; how many times you have submitted a job to a cluster only to learn 5 hours later that you can&rsquo;t convert a String to Double).</p>

<p>This example will show you how you can use Scalding with Hadoop 2.3 and how easy is to write a MapReduce job with few lines of Scala code.</p>

<h2>Build the project</h2>

<p>In our example we will transform a csv file to an other one with a filter step.
To build the project use:</p>

<p><code>./gradlew clean build</code> in the project library.</p>

<h2>Run the sample</h2>

<p>To run the sample with these parameters in local mode:</p>

<p><code>bash
yarn jar scalding-sample-0.1.jar com.sequenceiq.samples.scalding.CsvToCsvFilterJob --local --schema {YOUR_SCHEME} --input {INPUT} --type {TYPE} --operator {OPERATOR} --field {FILTER_FIELD} --operand {OPERAND} --output {OUTPUT_PATH}
</code></p>

<p>or if you want to run the exampke using HDFS then use:
<code>bash
yarn jar scalding-sample-0.1.jar com.sequenceiq.samples.scalding.CsvToCsvFilterJob --hdfs --schema {YOUR_SCHEME} --input {INPUT} --type {TYPE} --operator {OPERATOR} --field {FILTER_FIELD} --operand {OPERAND} --output {OUTPUT_PATH}
</code></p>

<p>To run the filtering example the parameters are like this:
<code>bash
yarn jar scalding-sample-0.1.jar com.sequenceiq.samples.scalding.CsvToCsvFilterJob --hdfs --schema id,name --input /input.csv --type int --operator eq --field id --operand 1 --output /output.csv
</code></p>

<p>The code looks extremely simple:</p>

<p>``` java
validation()
  input(args)</p>

<pre><code>.filter(filterableField) {field: String =&gt; createFilterCriterion(field)}
.write(output(args))
</code></pre>

<p>```</p>

<!-- more -->


<p>First there is a validation and in case of the input data is OK then we are doing a filtering with the specified criterias.
In this example (as in all our other examples) we are using Hadoop 2 &ndash; with the ability to submit Scalding jobs into a remote Hadoop 2 cluster. Note that Scalding depends on the Cascading library which does not support Hadoop 2 and there is no ability to submit jobs to a remote cluster &ndash; our example has removed the Hadoop 1 dependencies and lets you to submit jobs to any remote Hadoop 2 cluster.</p>

<p>``` java</p>

<pre><code>JobRunner.runJob(
    configurationService.getConfiguration(),
    new String[]{
        parameters..
    }
);
</code></pre>

<p>```
You can get the example project from our <a href="https://github.com/sequenceiq/sequenceiq-samples/tree/master/scalding-sample">GitHub</a> repository.</p>

<p>Should you have any Scalding or Scala questions or observations let us know.
Enjoy,
SequenceIQ</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop on Docker introduction]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/04/04/hadoop-docker-introduction/"/>
    <updated>2014-04-04T18:24:17+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/04/04/hadoop-docker-introduction</id>
    <content type="html"><![CDATA[<p>In the last few weeks we&rsquo;ve created and published several Docker images (<a href="https://github.com/sequenceiq/hadoop-docker">Hadoop</a>, <a href="https://github.com/sequenceiq/hoya-docker">Hoya</a>, <a href="https://github.com/sequenceiq/tez-docker">Tez</a>) to help you to quick-start with Hadoop and the latest innovations using YARN.
While many people have downloaded and started to use these preconfigured images we&rsquo;ve been asked to give a short introduction of what Docker is, and how one can build Docker images. Also during the Hadoop Summit in Amsterdem we have been inquired in particular about running Hadoop on Docker, so this post is our answer for all the requests we received.</p>

<p>Docker is an open-source engine that automates the deployment of any application as a lightweight, portable, self-sufficient container that will run virtually anywhere.</p>

<h2>Installation</h2>

<p>First install Docker with a package manager. On Ubuntu there is an easy way to start with by running a simple curl script which will do it for you:
<code>curl -s https://get.docker.io/ubuntu/ | sudo sh</code>.
Unfortunately Mac, Windows and some Linux distributions cannot natively run Docker (yet). At <a href="http://sequenceiq.com/">SequenceIQ</a> we develop on OSX and run a 3-6 node Hadoop mini cluster on our laptops. To overcome the limitation of running Docker natively
you will have to install <code>boot2docker</code>. It is a Tiny Core Linux made specifically to run Docker containers and weights less than 24MB memory.
Initialize <em>(boot2docker init)</em> and start <em>(boot2docker up)</em> and you can SSH into the VM <em>(boot2docker ssh, pass: tcuser)</em>.</p>

<p>To verify the installation let&rsquo;s test it: <code>docker run ubuntu /bin/echo hello docker</code>. Docker did a bunch of things within seconds:</p>

<ul>
<li>Downloaded the base image from the docker.io index</li>
<li>Created a new LXC container</li>
<li>Allocated a filesystem for it</li>
<li>Mounted a read-write layer</li>
<li>Allocated a network interface</li>
<li>Setup an IP for it, with network address translation</li>
<li>Executed a process inside the container</li>
<li>Captured the output and printed it</li>
</ul>


<p>You can run an interactive shell as well <code>docker run -i -t ubuntu /bin/bash</code> and use this shell as you would use any other shell.</p>

<p>While there are lots of different Docker images available we would like to share how to create your own images.</p>

<!-- more -->


<h2>Dockerfile</h2>

<p>The <code>Dockerfile</code> describes the build steps and it can be viewed as an image representation. They provide a simple syntax for building images and
they are a great way to automate and script the images creation. Dockerfile instructions look like this:
<code>
INSTRUCTION arguments
</code></p>

<h3>FROM</h3>

<p>Every Dockerfile has to start with the <code>FROM image</code> instruction which sets the base image for subsequent instructions (e.g. in our <a href="https://github.com/sequenceiq/hoya-docker">Hoya</a> and <a href="https://github.com/sequenceiq/tez-docker">Tez</a> images we used our <a href="https://github.com/sequenceiq/hadoop-docker">Hadoop</a> image as a base, while the Hadoop image was built on top of the <code>tianon/centos</code> base image).
A base image is built from a trusted build (more on this later) and in case of Hoya and Tez the base image was: <code>sequenceiq/hadoop-docker</code>. You can browse the available containers in the
<a href="https://index.docker.io/">Docker index</a>.</p>

<h3>RUN</h3>

<p>The next instruction is usually the <code>RUN command</code>. This will execute any commands on the current image and commit the results. The resulting committed image
will be used for the next step in the Dockerfile. Example: RUN yum install -y openssh-server. One important thing to keep in mind is that the
following set of instructions will not act as we would like:
<code>
RUN cd /usr/local  
RUN mkdir apple  
</code>
This will create an apple folder in the root directory. Surprised, huh? The reason of this that the RUN command is equivalent to the docker commands:
docker run image command + docker commit container_id, where the image would be replaced automatically with the current image,
and container_id would be the result of the previous RUN instruction. But it doesn&rsquo;t mean it can&rsquo;t be done:
<code>
RUN cd /usr/local &amp;&amp; mkdir apple
</code></p>

<h3>ADD</h3>

<p>The <code>ADD from to</code> command will copy the specified file into the container. Example:
ADD data.xml /usr/local/data.xml. In this case the data.xml is in the same directory as the Dockerfile. After this command you can rely on that this file
is present in the container and you can use it as well: RUN rm /usr/local/data.xml.</p>

<h3>EXPOSE</h3>

<p>The <code>EXPOSE port</code> instruction sets ports to be exposed to the host when running the image. Example: EXPOSE 8080 80 22 50070</p>

<h3>ENV</h3>

<p>Setting an environment variable by running a RUN export KEY=value won&rsquo;t work in dockerland. Instead you can use the <code>ENV key value</code> instruction.
Example: ENV JAVA_HOME /usr/java/default</p>

<h3>ENTRYPOINT</h3>

<p>The <code>ENTRYPOINT [command]</code> instruction permits you to trigger a command as soon as the container starts. Example: ENTRYPOINT [&ldquo;echo&rdquo;, &ldquo;Whale you be my container&rdquo;]</p>

<p>There are more instructions, but these are enough to start with and build your own images.</p>

<h2>Build &amp; Trusted build</h2>

<p>Once the Dockerfile is ready you can build it. If the file is in the current directory build it with <code>docker build .</code> (-t name to TAG the image). It&rsquo;s possible
to create trusted builds. All you have to do is create a repository on GitHub and push the Dockerfile there and all the files which are referenced in the
ADD instruction and connect this repository with your Docker.io account. Docker.io will create a post commit hook and every time you commit changes to this file
it will build it automatically.</p>

<h2>Usage</h2>

<p>Use this environment variable to make things easier: export DOCKER_HOST=tcp://localhost:4243. Few frequently used commands:</p>

<ul>
<li>List of your local images: docker images</li>
<li>List of running containers: docker ps</li>
<li>List of all containers: docker ps -a</li>
</ul>


<p>After you built your image it should show in the image list, and ready to use. Run it with <code>docker run -i -t -P image_name /bin/bash</code>. The -P variable will
publish all exposed ports to the host interfaces.</p>

<h2>Complete example</h2>

<p>As a reference check out our Hadoop 2.3 based <a href="https://github.com/sequenceiq/hadoop-docker">Dockerfile</a>.</p>

<h2>OSX Tweaks</h2>

<h3>Passwordless ssh</h3>

<p>On OSX it&rsquo;s quite tedious to always type tcuser password when you ssh into boot2docker. You can install your public key with a oneliner. You have to set the
KEYCHAIN variable to your <a href="http://keychain.io">Keychain.io</a> registered email.
<code>
(export KEYCHAIN=&lt;email&gt;; curl -L j.mp/chain2docker|bash)
</code>
If you restart boot2docker, you have to run this command again, for a passwordless ssh. To install your public ssh key into keychain is as simple as:
<code>
curl -s ssh.keychain.io/&lt;email&gt;/upload | bash
</code>
than you will receive a confirmation email, that&rsquo;s all.</p>

<h3>Expose ports from boot2docker to host</h3>

<p>Let&rsquo;s say you have a docker image starting Hadoop Name Node on port 50070. When you start 3 images you will get something like this:</p>

<ul>
<li>instance1: 50070 &ndash;> 49153</li>
<li>instance1: 50070 &ndash;> 49154</li>
<li>instance1: 50070 &ndash;> 49155</li>
</ul>


<p>But all those 4915X ports are only available when you are inside of boot2docker. Now if you forward all 49XXX ports straight to to your host,
you can reach the namenodes in your browser running on your mac as: <a href="http://localhost:4915X">http://localhost:4915X</a>
<code>
boot2docker stop
for i in {49000..49900}; do
 echo -n .
 VBoxManage modifyvm "boot2docker-vm" --natpf1 "tcp-port$i,tcp,,$i,,$i";
 VBoxManage modifyvm "boot2docker-vm" --natpf1 "udp-port$i,udp,,$i,,$i";
done
boot2docker up
</code>
That&rsquo;s it. Hope this helps you to start with building your own Docker images. Let us know how it goes, we are happy to help you quick start Hadoop on Docker.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop 2.3 with docker]]></title>
    <link href="http://blog.sequenceiq.com/blog/2014/03/19/hadoop-2-dot-3-with-docker/"/>
    <updated>2014-03-19T03:54:09+00:00</updated>
    <id>http://blog.sequenceiq.com/blog/2014/03/19/hadoop-2-dot-3-with-docker</id>
    <content type="html"><![CDATA[<p>You want to try out hadoop 2.3? Go to the zoo and <a href="http://sethgodin.typepad.com/seths_blog/2005/03/dont_shave_that.html">shave a yak</a>.
Or simply just use <a href="https://www.docker.io/">docker</a>.</p>

<p><code>
docker run -i -t sequenceiq/hadoop-docker /etc/bootstrap.sh -bash
</code></p>

<h2>Testing</h2>

<p>```bash</p>

<h1>start ssh and hdfs</h1>

<p>cd $HADOOP_PREFIX</p>

<h1>run the mapreduce</h1>

<p>bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.3.0.jar grep input output &lsquo;dfs[a-z.]+&rsquo;</p>

<h1>check the output</h1>

<p>bin/hdfs dfs -cat output/*
```</p>

<!-- more -->


<h2>Yak shaving an elefant</h2>

<p>I had problems installing hadoop 2.3 and by googling i stumbled upon this <a href="http://mail-archives.apache.org/mod_mbox/hadoop-mapreduce-user/201403.mbox/%3C53192FD4.2040003@oss.nttdata.co.jp%3E">email thread</a>,
which references an <a href="http://aajisaka.github.io/hadoop-project/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation">alternative hadoop docs</a> deployed on github.</p>

<p>By following that description i run into an other issue:
hadoop is delivered with 32 bit native libraries. No big deal &hellip;</p>

<h2>Hadoop native libraries</h2>

<p>Of course there is an official <a href="http://hadoop.apache.org/docs/r2.3.0/hadoop-project-dist/hadoop-common/NativeLibraries.html">Native Libraries Guide</a> it instructs you
to simple download the sources and <strong>mvn package</strong>. But than you face a new issue: missing <em>protobuf</em>. Eeeasy &hellip;</p>

<h2>Protobuf 2.5</h2>

<p>Unfortunately <strong>yum install protobuf</strong> installs an older 2.3 version, which is close but no cigar.
 So you download protobuf source, and <strong>./configure &amp;&amp; make &amp;&amp; make install</strong></p>

<p>To succeed on that one you have to install a couple of development packages, and there you go.</p>

<h2>Bintray</h2>

<p>I wanted to save you those steps so created a binary distro of the native libs
compiled with 64 bit CentOS. So I created <a href="https://bintray.com/sequenceiq/sequenceiq-bin/hadoop-native-64bit/2.3.0/view/files">Bintray rÌ¨epo</a>. Enjoy</p>

<h2>Automate everything</h2>

<p>As I&rsquo;m an automation fetishist, a Docker file was created, and released in the official <a href="https://index.docker.io/u/sequenceiq/hadoop-docker/">docker repo</a></p>
]]></content>
  </entry>
  
</feed>
